{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Artifical Neural Network\n",
    "- biological neural network를 모방한 computing system이다.\n",
    "- layer가 두개 이상 쌓인 형태는 Deep Neural Network라고 한다.\n",
    "- 간단히 생각하면 아래 그림과 같이 linear classifier를 여러번 이어놓은 형태를 의미한다. <br/>hidden node에서 input의 feature들을 우리가 지정한 hidden layer의 개수만큼으로 표현해주고 이러한 linear function을 여러번 만들어서 최종 output을 추출해내는 형태이다. <br/>\n",
    "    이때 neural network가 어떤 식으로 feature들을 가공하는지 알 수 없기 때문에 blackbox algorithm이라고 하며, 기계가 사람이 알 수 없는 방식으로 필요한 feature들을 뽑아준다는 점에서 deep learning이 기존의 머신러닝 알고리즘들과 가장 큰 차이점을 보인다고 할 수 있다.<br/> \n",
    "<img src = \"picture/neural_network.png\">\n",
    "- 그림에서 보이는 각각의 선들이 weight를 의미한다고 할 수 있다.<br/> Training과정에서 input을 넣어서 output을 계산하고 score를 계산하는 과정을 **forward propagation** 이라고 하며, loss function을 이용해서 weight를 loss를 줄이는 방향으로 update를 하는 과정을 **back propagation**이라고 한다.<br/> 이러한 과정으로 한번의 forward propagation이랑 back propagation 과정을 한번 거치는 것을 1 **epoch**라고 한다.<br/><br/>\n",
    "- deep learning model은 다양한 hyperparameter에 의해서 영향을 받는다. 이러한 hyperparamter의 종류는 다음과 같다.\n",
    "    - learning rate : weight를 얼마만큼 이동하면서 update할 것인지\n",
    "    - number of hidden layer : layer를 얼마나 쌓을 것인지\n",
    "    - number of hidden node for each layer : 각 hidden layer의 node를 몇개씩 지정해 줄것인지\n",
    "    - regularization : l2또는 l1 regularization 을 얼마만큼 적용해줄 것인지\n",
    "    - dropout rate : dropout시키는 비율을 얼마로 할것인지\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network의 원리\n",
    "## Forward pass\n",
    "\n",
    "input으로 (N,D)의 X가 주어졌다고 했을 때, hidden layer가 하나인 forward pass의 계산과정은 다음과 같다. <br/>\n",
    "각 layer마다 linear classifier와 동일하게 wx+b 함수를 계산하고, activation함수를 이용해서 output이 0또는 1나올 수 있게 조정해준다.<br/>\n",
    "여기서는 activation function으로 ReLU함수를 이용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(self,x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "# parameter W와 b 불러오기\n",
    "W1, b1 = self.params['W1'], self.params['b1']\n",
    "W2, b2 = self.params['W2'], self.params['b2']\n",
    "N, D = X.shape\n",
    "\n",
    "# Compute the forward pass \n",
    "scores = None\n",
    "u1 = X.dot(W1)+b1\n",
    "z1 = self.relu(u1)\n",
    "scores = z1.dot(W2)+b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute loss\n",
    "계산해서 얻은 output이 실제랑 얼마나 잘 맞는지(해당 모델이 얼마나 잘 학습되었는지 확인하기 위해 loss를 계산한다.<br/>\n",
    "loss function으로는 대표적으로 svm loss(hinge loss)와 softmax function을 많이 이용한다.<br/>\n",
    "여기서는 svm loss를 활용하였다. 그리고 loss에 regularization term을 더해줌으로써 overfitting을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the loss\n",
    "loss = None\n",
    "        \n",
    "prob = np.exp(scores)/np.sum(np.exp(scores),axis=1,keepdims=True)\n",
    "        \n",
    "loss = np.sum(-np.log(prob[range(N),y]))/N\n",
    "loss += 0.5 * reg * (np.sum(W1**2)+np.sum(W2**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax loss function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_train = X.shape[0]\n",
    "    num_classes = W.shape[1]\n",
    "    \n",
    "    score=X.dot(W)\n",
    "    stable_score=score-np.array(num_classes*[np.max(score,axis=1),]).T\n",
    "    prob = np.exp(stable_score)/np.sum(np.exp(stable_score),axis=1,keepdims=True)\n",
    "    \n",
    "    loss=np.sum(-np.log(prob[np.arange(0,prob.shape[0]),y]))\n",
    "    loss=loss/num_train + 0.5 * reg * np.sum(W*W)  \n",
    "    \n",
    "    prob[np.arange(0,prob.shape[0]),y] -= 1\n",
    "    \n",
    "    dW = prob.T.dot(X).T\n",
    "    \n",
    "    dW /= num_train\n",
    "    dW += reg*W\n",
    "\n",
    "    pass\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVM Loss **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "    Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "    pass\n",
    "  \n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    scores = X.dot(W)\n",
    "\n",
    "    correct=scores[np.arange(0,scores.shape[0]),y]\n",
    "    #dW[:,y[i]]+=-2*X[i]\n",
    "        \n",
    "    correct_score=np.array([correct,]*num_classes).T\n",
    "    \n",
    "    margin=scores-correct_score+1\n",
    "    loss=np.sum(margin*(margin>0)*(margin!=1))/num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "     \n",
    "    pass\n",
    "\n",
    "    sum_or_not=np.array((margin>0)*(margin!=1),dtype=float)\n",
    "    sum_or_not[np.arange(0,scores.shape[0]),y]=-np.sum(sum_or_not,axis=1)\n",
    "    \n",
    "    dW=np.dot(X.T,sum_or_not)\n",
    "    \n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "    \n",
    "    pass\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "back propagation은 gradient descent optimization방법을 이용해서 weight를 update해나가는 과정을 의미한다 <br/>\n",
    "**Gradient Descent**란 가설함수의 local minimum지점을 찾기 위해서 현재 지점의 gradient(미분값)의 음수값에 비례하게 이동해가면서 minimum을 찾아나가는 optimization algorithm이다.  \n",
    "<img src = \"picture/sgd.png\">\n",
    "따라서 각각의 parameter(W1, b1, W2, b2)에 대해서 weight를 update하기 위해서는 loss function에 대해서 각각의 parameter에 대한 편미분(partial derivative)값을 구해줌으로써 그 값에 비례하게 weight를 update한다.\n",
    "$$ new_w = w - (learning\\,rate)* \\frac{\\delta\\,loss}{\\delta \\,w} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward pass: compute gradients\n",
    "dscore = prob.copy()\n",
    "dscore[np.arange(0,prob.shape[0]),y] -= 1\n",
    "dscore = dscore/N\n",
    "        \n",
    "grads = {}\n",
    "grads['W2'] = z1.T.dot(dscore) + reg*W2\n",
    "grads['b2'] = np.sum(dscore,axis=0) + reg*b2\n",
    "       \n",
    "tmp = dscore.dot(W2.T)\n",
    "tmp[z1<=0]=0\n",
    "grads['W1'] = X.T.dot(tmp) +reg*W1\n",
    "grads['b1'] = np.sum(tmp,axis=0) + reg*b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 과정을 여러 번(iteration) 반복해주면 weight가 training data를 잘 설명해 줄 수 있는 값으로 최적화되고 training과정이 끝나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tools import rmse_cal,mae_cal,cor_cal,mean_cal,frange,\\\n",
    "                    accuracy,precision,recall,aupr,f1_score,make_binary\n",
    "from validation import classification_cv,regression_cv,external_val_reg,\\\n",
    "                        external_val_classif, test_preprocessing, \\\n",
    "                        draw_roc,cal_external_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset=pd.read_table('data/breast_cancer_svc.tsv',sep='\\t')\n",
    "\n",
    "input_data=dataset.iloc[:,1:].transpose()\n",
    "X_data=input_data.iloc[:,:-1].values\n",
    "y_data=input_data.iloc[:,-1]\n",
    "y_data=make_binary('normal','cancer',y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variable 정의\n",
    "X_dim = X_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input 형태 정의\n",
    "x = tf.placeholder(tf.float32, [None, X_dim])\n",
    "y = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# create fully connected layer\n",
    "def fully_connected(input_layer, weights, biases):\n",
    "    layer = tf.add(tf.matmul(input_layer, weights), biases)\n",
    "    return(tf.nn.relu(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Dense\"):\n",
    "    w = tf.Variable(ones[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
