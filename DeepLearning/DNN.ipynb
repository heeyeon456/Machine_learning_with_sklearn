{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [1.Multi-layer perceptron](#session1)\n",
    "* [2.Training the network](#session2)\n",
    "* [3.Example code](#session3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mult-Layer Perceptron\n",
    "---\n",
    "<a id='#session1'></a>\n",
    "- biological neural network를 모방한 computing system이다.\n",
    "- 하나의 layer가 다중 neuron으로 구성된 형태는 MLP(multi-layer perceptron)이라고 하며, layer가 두개 이상 쌓인 형태는 Deep Neural Network라고 한다.\n",
    "- 간단히 생각하면 아래 그림과 같이 linear classifier를 여러번 이어놓은 형태를 의미한다. <br/>hidden node에서 input의 feature들을 우리가 지정한 hidden layer의 개수만큼으로 표현해주고 이러한 linear function을 여러번 만들어서 최종 output을 추출해내는 형태이다. <br/>\n",
    "    이때 neural network가 어떤 식으로 feature들을 가공하는지 알 수 없기 때문에 blackbox algorithm이라고 하며, 기계가 사람이 알 수 없는 방식으로 필요한 feature들을 뽑아준다는 점에서 deep learning이 기존의 머신러닝 알고리즘들과 가장 큰 차이점을 보인다고 할 수 있다.<br/> \n",
    "![ann](picture/neural_network.png)\n",
    "<br/><br/>\n",
    "- 그림에서 보이는 각각의 선들이 weight를 의미한다고 할 수 있다.<br/> Training과정에서 input을 넣어서 output을 계산하고 score를 계산하는 과정을 **forward propagation** 이라고 하며, loss function을 이용해서 weight를 loss를 줄이는 방향으로 update를 하는 과정을 **back propagation**이라고 한다.<br/> 이러한 과정으로 한번의 forward propagation이랑 back propagation 과정을 한번 거치는 것을 1 **epoch**라고 한다.<br/><br/>\n",
    "- deep learning model은 다양한 hyperparameter에 의해서 영향을 받는다. 이러한 hyperparamter의 종류는 다음과 같다.\n",
    "    - learning rate : weight를 얼마만큼 이동하면서 update할 것인지\n",
    "    - number of hidden layer : layer를 얼마나 쌓을 것인지\n",
    "    - number of hidden node for each layer : 각 hidden layer의 node를 몇개씩 지정해 줄것인지\n",
    "    - regularization : l2또는 l1 regularization 을 얼마만큼 적용해줄 것인지\n",
    "    - dropout rate : dropout시키는 비율을 얼마로 할것인지\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#session2'></a>\n",
    "# 2. Training the Network\n",
    "---\n",
    "## 2-1. Forward pass\n",
    "\n",
    "input으로 (N,D)의 X가 주어졌다고 했을 때, hidden layer가 하나인 forward pass의 계산과정은 다음과 같다. <br/>\n",
    "각 layer마다 linear classifier와 동일하게 wx+b 함수를 계산하고, activation함수를 이용해서 output이 0또는 1나올 수 있게 조정해준다.<br/>\n",
    "여기서는 activation function으로 ReLU함수를 이용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(self,x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "# parameter W와 b 불러오기\n",
    "W1, b1 = self.params['W1'], self.params['b1']\n",
    "W2, b2 = self.params['W2'], self.params['b2']\n",
    "N, D = X.shape\n",
    "\n",
    "# Compute the forward pass \n",
    "scores = None\n",
    "u1 = X.dot(W1)+b1\n",
    "z1 = self.relu(u1)\n",
    "scores = z1.dot(W2)+b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute loss\n",
    "* 계산해서 얻은 output이 실제랑 얼마나 잘 맞는지(해당 모델이 얼마나 잘 학습되었는지 확인하기 위해 loss를 계산한다.\n",
    "* net의 마지막 output 결과 class개수 만큼의 값이 나오지만 이 값들의 합은 1이 아니므로 이를 합이 1인 평균의 형태로 맞춰주기 위하여 softmax function을 이용하여 진짜 클래스를 얼마의 확률로 맞추는 지에 대한 loss함수를 계산하게 된다.\n",
    "* 그리고 loss에 regularization term을 더해줌으로써 overfitting을 방지한다.\n",
    "* loss function으로는 대표적으로 svm loss(hinge loss)와 softmax function을 이용한 cross entropy를 많이 이용한다.\n",
    "  1) softmax cross entropy<br>\n",
    "  2) hinge loss(svm loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss\n",
    "# 합이 1인 확률의 형태로 만든다.\n",
    "# exp를 취하는 이유는 나중에 loss를 계산할때 log를 취해 뺄셈의 형태로 만들기 위해서 이다.\n",
    "\n",
    "loss = None\n",
    "        \n",
    "prob = np.exp(scores)/np.sum(np.exp(scores),axis=1,keepdims=True)\n",
    "        \n",
    "loss = np.sum(-np.log(prob[range(N),y]))/N\n",
    "loss += 0.5 * reg * (np.sum(W1**2)+np.sum(W2**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Softmax loss function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_train = X.shape[0]\n",
    "    num_classes = W.shape[1]\n",
    "    \n",
    "    # 각 class대한 score계산\n",
    "    score=X.dot(W)\n",
    "    #값이 너무 커지지 않게 줄여주는 과정인듯\n",
    "    stable_score=score-np.array(num_classes*[np.max(score,axis=1),]).T\n",
    "    #전체 class에 대한 합으로 나눔\n",
    "    prob = np.exp(stable_score)/np.sum(np.exp(stable_score),axis=1,keepdims=True)\n",
    "    \n",
    "    #진짜 클래스에 대한 probability값\n",
    "    loss=np.sum(-np.log(prob[np.arange(0,prob.shape[0]),y]))\n",
    "    loss=loss/num_train + 0.5 * reg * np.sum(W*W)  \n",
    "    \n",
    "    prob[np.arange(0,prob.shape[0]),y] -= 1\n",
    "    \n",
    "    # stochastic gradient descent를 위해 loss에 대한 gradient계산\n",
    "    dW = prob.T.dot(X).T\n",
    "    \n",
    "    dW /= num_train\n",
    "    dW += reg*W\n",
    "\n",
    "    pass\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVM Loss **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "    Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "    pass\n",
    "  \n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    scores = X.dot(W)\n",
    "    # 진짜 class에 대한 score값\n",
    "    correct=scores[np.arange(0,scores.shape[0]),y]\n",
    "    #dW[:,y[i]]+=-2*X[i]  \n",
    "    correct_score=np.array([correct,]*num_classes).T\n",
    "    \n",
    "    #진짜 class의 score와 다른 class들과의 차이(margin) 계산\n",
    "    margin=scores-correct_score+1\n",
    "    \n",
    "    #다른 class들과의 margin이 커지는 방향으로 loss 계산\n",
    "    loss=np.sum(margin*(margin>0)*(margin!=1))/num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "     \n",
    "    pass\n",
    "\n",
    "    sum_or_not=np.array((margin>0)*(margin!=1),dtype=float)\n",
    "    sum_or_not[np.arange(0,scores.shape[0]),y]=-np.sum(sum_or_not,axis=1)\n",
    "    \n",
    "    dW=np.dot(X.T,sum_or_not)\n",
    "    \n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "    \n",
    "    pass\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2-2. Back Propagation\n",
    "back propagation은 gradient descent optimization방법을 이용해서 weight를 update해나가는 과정을 의미한다 <br/>\n",
    "**Gradient Descent**란 가설함수의 local minimum지점을 찾기 위해서 현재 지점의 gradient(미분값)의 음수값에 비례하게 이동해가면서 minimum을 찾아나가는 optimization algorithm이다.  \n",
    "![sgd](picture/sgd.png)\n",
    "\n",
    "따라서 각각의 parameter(W1, b1, W2, b2)에 대해서 weight를 update하기 위해서는 loss function에 대해서 각각의 parameter에 대한 편미분(partial derivative)값을 구해줌으로써 그 값에 비례하게 weight를 update한다.\n",
    "$$ new_w = w - (learning\\,rate)* \\frac{\\delta\\,loss}{\\delta \\,w} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass: compute gradients\n",
    "# 원래 parameter를 계산한 gradient만큼 update\n",
    "dscore = prob.copy()\n",
    "dscore[np.arange(0,prob.shape[0]),y] -= 1\n",
    "dscore = dscore/N\n",
    "        \n",
    "grads = {}\n",
    "grads['W2'] = z1.T.dot(dscore) + reg*W2\n",
    "grads['b2'] = np.sum(dscore,axis=0) + reg*b2\n",
    "       \n",
    "tmp = dscore.dot(W2.T)\n",
    "tmp[z1<=0]=0\n",
    "grads['W1'] = X.T.dot(tmp) +reg*W1\n",
    "grads['b1'] = np.sum(tmp,axis=0) + reg*b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 과정을 여러 번(iteration) 반복해주면 weight가 training data를 잘 설명해 줄 수 있는 값으로 최적화되고 training과정이 끝나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#session3'></a>\n",
    "# 3. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tools import rmse_cal,mae_cal,cor_cal,mean_cal,frange,\\\n",
    "                    accuracy,precision,recall,aupr,f1_score,make_binary\n",
    "from validation import classification_cv,regression_cv,external_val_reg,\\\n",
    "                        external_val_classif, test_preprocessing, \\\n",
    "                        draw_roc,cal_external_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "dataset=pd.read_table('../data/breast_cancer_svc.tsv',sep='\\t')\n",
    "\n",
    "input_data=dataset.iloc[:,1:].transpose()\n",
    "X_data=input_data.iloc[:,:-1].values\n",
    "y_data=input_data.iloc[:,-1]\n",
    "y_data=make_binary('normal','cancer',y_data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.1, random_state=42)\n",
    "\n",
    "y_train = enc.fit_transform(y_train.values.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable 정의\n",
    "X_dim = X_train.shape[1]\n",
    "input_n = X_train.shape[0]\n",
    "hidden_1 = 200\n",
    "hidden_2 = 100\n",
    "output_size = 2\n",
    "lr = 1e-04\n",
    "max_epoch = 50\n",
    "dropout_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fully connected layer\n",
    "def dense(input_layer,layer_size):\n",
    "    \n",
    "    init = tf.contrib.layers.xavier_initializer()\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "    \n",
    "    # Batchnorm settings\n",
    "    #training_phase = tf.placeholder(tf.bool, phase, name='training_phase')\n",
    "    \n",
    "    HiddenLayer = tf.layers.dense(inputs = input_layer, units = layer_size, \n",
    "                              activation=None, # Batchnorm comes before nonlinear activation\n",
    "                              use_bias=False, # Note that no bias unit is used in batchnorm\n",
    "                              kernel_initializer=init, kernel_regularizer = regularizer)\n",
    "    \n",
    "    HiddenLayer = tf.layers.batch_normalization(HiddenLayer)\n",
    "    HiddenLayer = tf.nn.relu(HiddenLayer)\n",
    "    \n",
    "    return HiddenLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# input 형태 정의\n",
    "X = tf.placeholder(tf.float32, [None, X_dim])\n",
    "y = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "with tf.name_scope(\"DNN\"):\n",
    "    \n",
    "    keep_proba = tf.placeholder(tf.float32, None, name='keep_proba')\n",
    "\n",
    "    \n",
    "    hidden_layer1 = dense(X,hidden_1)\n",
    "    hidden_layer2 = dense(hidden_layer1, hidden_2)\n",
    "    hidden_layer2 = tf.nn.dropout(hidden_layer2, keep_prob=dropout_prob)\n",
    "    output_layer = dense(hidden_layer2, output_size)\n",
    "     \n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=output_layer,labels=y)\n",
    "    cost = tf.reduce_mean(loss, name = 'loss')\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "    training = optimizer.minimize(loss,name='training')\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(tf.argmax(y,1), tf.argmax(output_layer,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name='eval')\n",
    "    \n",
    "    \n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch : ', 1, '| Training Accuracy:', 0.81946403, '  Validation Accuracy:', 0.82278484)\n",
      "('Epoch : ', 2, '| Training Accuracy:', 0.83215803, '  Validation Accuracy:', 0.8101266)\n",
      "('Epoch : ', 3, '| Training Accuracy:', 0.83921021, '  Validation Accuracy:', 0.8101266)\n",
      "('Epoch : ', 4, '| Training Accuracy:', 0.83921027, '  Validation Accuracy:', 0.8607595)\n",
      "('Epoch : ', 5, '| Training Accuracy:', 0.84767276, '  Validation Accuracy:', 0.89873421)\n",
      "('Epoch : ', 6, '| Training Accuracy:', 0.85754585, '  Validation Accuracy:', 0.88607597)\n",
      "('Epoch : ', 7, '| Training Accuracy:', 0.85895628, '  Validation Accuracy:', 0.88607597)\n",
      "('Epoch : ', 8, '| Training Accuracy:', 0.86036676, '  Validation Accuracy:', 0.89873415)\n",
      "('Epoch : ', 9, '| Training Accuracy:', 0.85895634, '  Validation Accuracy:', 0.87341774)\n",
      "('Epoch : ', 10, '| Training Accuracy:', 0.87023979, '  Validation Accuracy:', 0.88607597)\n",
      "('Epoch : ', 11, '| Training Accuracy:', 0.86600846, '  Validation Accuracy:', 0.89873421)\n",
      "('Epoch : ', 12, '| Training Accuracy:', 0.87306064, '  Validation Accuracy:', 0.89873421)\n",
      "('Epoch : ', 13, '| Training Accuracy:', 0.87165022, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 14, '| Training Accuracy:', 0.8730607, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 15, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.89873421)\n",
      "('Epoch : ', 16, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 17, '| Training Accuracy:', 0.8730607, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 18, '| Training Accuracy:', 0.87023979, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 19, '| Training Accuracy:', 0.87447107, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 20, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 21, '| Training Accuracy:', 0.87729204, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 22, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 23, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 24, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 25, '| Training Accuracy:', 0.87447107, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 26, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 27, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 28, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 29, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 30, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 31, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 32, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 33, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 34, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 35, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 36, '| Training Accuracy:', 0.87588143, '  Validation Accuracy:', 0.89873415)\n",
      "('Epoch : ', 37, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 38, '| Training Accuracy:', 0.87447107, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 39, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 40, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 41, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 42, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 43, '| Training Accuracy:', 0.87447107, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 44, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 45, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 46, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 47, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n",
      "('Epoch : ', 48, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139245)\n",
      "('Epoch : ', 49, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.89873421)\n",
      "('Epoch : ', 50, '| Training Accuracy:', 0.87588155, '  Validation Accuracy:', 0.91139239)\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    while epoch_count < max_epoch :\n",
    "        \n",
    "        sess.run(training,feed_dict={X:X_train,y:y_train})\n",
    "        accu_train = accuracy.eval(feed_dict={X:X_train,y:y_train})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:X_test,y:y_test})\n",
    "        \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            \n",
    "        epoch_count+=1\n",
    "        \n",
    "        print('Epoch : ',epoch_count, '| Training Accuracy:',accu_train,'  Validation Accuracy:',accu_valid)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 64\n",
    "dropout_rate = 0.5\n",
    "batch_size = 25\n",
    "\n",
    "epochs = 20\n",
    "train_samples = X_train.shape[0]\n",
    "validation_samples = X_test.shape[0]\n",
    "x_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 294)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(hidden_1,activation = 'relu',input_dim = x_dim))\n",
    "model.add(Dense(hidden_2, activation = 'relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 709 samples, validate on 79 samples\n",
      "Epoch 1/20\n",
      "709/709 [==============================] - 0s - loss: 0.4199 - acc: 0.8477 - val_loss: 0.2824 - val_acc: 0.9114\n",
      "Epoch 2/20\n",
      "709/709 [==============================] - 0s - loss: 0.3130 - acc: 0.8773 - val_loss: 0.2382 - val_acc: 0.9114\n",
      "Epoch 3/20\n",
      "709/709 [==============================] - 0s - loss: 0.2829 - acc: 0.8935 - val_loss: 0.2091 - val_acc: 0.9430\n",
      "Epoch 4/20\n",
      "709/709 [==============================] - 0s - loss: 0.2266 - acc: 0.9210 - val_loss: 0.1707 - val_acc: 0.9620\n",
      "Epoch 5/20\n",
      "709/709 [==============================] - 0s - loss: 0.1712 - acc: 0.9401 - val_loss: 0.1253 - val_acc: 0.9747\n",
      "Epoch 6/20\n",
      "709/709 [==============================] - 0s - loss: 0.1385 - acc: 0.9528 - val_loss: 0.0873 - val_acc: 0.9684\n",
      "Epoch 7/20\n",
      "709/709 [==============================] - 0s - loss: 0.0766 - acc: 0.9817 - val_loss: 0.0517 - val_acc: 0.9810\n",
      "Epoch 8/20\n",
      "709/709 [==============================] - 0s - loss: 0.0730 - acc: 0.9739 - val_loss: 0.0389 - val_acc: 0.9937\n",
      "Epoch 9/20\n",
      "709/709 [==============================] - 0s - loss: 0.0452 - acc: 0.9908 - val_loss: 0.0302 - val_acc: 0.9873\n",
      "Epoch 10/20\n",
      "709/709 [==============================] - 0s - loss: 0.0264 - acc: 0.9958 - val_loss: 0.0204 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "709/709 [==============================] - 0s - loss: 0.0276 - acc: 0.9937 - val_loss: 0.0176 - val_acc: 0.9937\n",
      "Epoch 12/20\n",
      "709/709 [==============================] - 0s - loss: 0.0198 - acc: 0.9972 - val_loss: 0.0186 - val_acc: 0.9873\n",
      "Epoch 13/20\n",
      "709/709 [==============================] - 0s - loss: 0.0159 - acc: 0.9993 - val_loss: 0.0110 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "709/709 [==============================] - 0s - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0132 - val_acc: 0.9937\n",
      "Epoch 15/20\n",
      "709/709 [==============================] - 0s - loss: 0.0103 - acc: 0.9993 - val_loss: 0.0235 - val_acc: 0.9873\n",
      "Epoch 16/20\n",
      "709/709 [==============================] - 0s - loss: 0.0091 - acc: 0.9986 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "709/709 [==============================] - 0s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "709/709 [==============================] - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "709/709 [==============================] - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0143 - val_acc: 0.9873\n",
      "Epoch 20/20\n",
      "709/709 [==============================] - 0s - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efde42f8650>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should install h5py\n",
    "\n",
    "model.save_weights('models/dnn_weight1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Batch Normalization\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient vanishing\n",
    "- neural network의 layer가 깊어질수록 gradient값이 0으로 수렴하는 문제점을 의미한다. \n",
    "- gradient descent방법을 이용한 parameter update는 parameter value의 작은 변화(미분값)이 network output에 얼마나 영향을 미칠지를 반영하여 parameter를 update한다.\n",
    "- 이 변화량은 network가 깊어질수록 미분값이 0으로 수렴해버린다는 문제점이 있다. \n",
    "- 이러한 문제점이 발생하는 원인은 sigmoid나 tanh같은 기존의 activation function을 선택하면 매우 nonlinear한 방식으로 input을 (0,1) 또는 (-1,1)사이로 넣어버린다(squash)\n",
    "- 이러한 문제점을 해결하는 대표적인 방법은 activation function으로 ReLU를 사용하는 것이다.\n",
    "- batch normalization도 이러한 방법을 해결하는 대표적인 방법 중 하나이다.<br/>\n",
    "[출처](http://ydseo.tistory.com/41)\n",
    "\n",
    "### Batch normalization\n",
    "- batch normalizatio은 gradient vanising현상이 일어하는 원인 중 하나가 'Interval Convariance Shift'라고 주장한다.\n",
    "- **Internal Covariant Shift**는 network의 각 층마다 input distribution이 달라지는 현상을 의미한다.\n",
    "- 이러한 현상을 막기 위해서는 간단하게 각 층의 input의 distribution을 표준정규분포로 normalize시키는 방법을 생각할 수 있다.\n",
    "<br/>\n",
    "- pseudo code<br/><br/>\n",
    "![bn](picture/bn.png)\n",
    "<br/><br/>\n",
    "- 알고리즘을 보면 각 input값을 mini batch의 평균과 표준편차를 가지고 normalize해주고, 이를 beta(shift factor)와 gamma(scale factor)값을 이용해 변형시켜준 결과이다.\n",
    "- test set에는 training set에서 계산해 놓은 이동평균으로 normalize한 뒤 마찬가지로 beta와 gamma로 변형시켜준다. \n",
    "### Backpropagation\n",
    "<br/>\n",
    "- Computational graph of batch normalization layer\n",
    "<br/><br/>\n",
    "![bn](picture/BNcircuit.png)\n",
    "<br/><br/>\n",
    "- update할 파라미터는 beta, gamma, mu, \n",
    "[출처](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    \n",
    "    if mode == 'train':\n",
    "        sample_mean = np.mean(x)\n",
    "        sample_var = np.std(x)**2\n",
    "        norm_x = (x-sample_mean)/np.sqrt(sample_var + eps)\n",
    "        out = gamma*norm_x+beta\n",
    "        \n",
    "        running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "        running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "        \n",
    "        cache = (x, sample_mean, sample_var, norm_x, beta, gamma, eps)\n",
    "\n",
    "    elif mode == 'test':\n",
    "        norm_x = (x-running_mean)/np.sqrt(running_var + eps)\n",
    "        scaled_x = gamma*norm_x+beta\n",
    "        out=scaled_x\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "    \n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for batch normalization.\n",
    "\n",
    "    For this implementation, you should write out a computation graph for\n",
    "    batch normalization on paper and propagate gradients backward through\n",
    "    intermediate nodes.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from batchnorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    \n",
    "    (x, sample_mean, sample_var, norm_x, beta, gamma, eps) = cache\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    dbeta = np.sum(dout, axis = 0)\n",
    "    dgamma = np.sum(norm_x*dout, axis=0)\n",
    "    dnorm_x = gamma*dout\n",
    "    dsample_mean = np.sum(-1/np.sqrt(sample_var+eps)* dnorm_x, axis = 0) + 1.0/N*sample_var *np.sum(-2*(x-sample_mean), axis = 0) \n",
    "    dx = 1/np.sqrt(sample_var+eps)*dnorm_x + sample_var*2.0/N*(x-sample_mean) + 1.0/N*dsample_mean\n",
    "\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구현 이슈\n",
    "tensorflow에서 batch normalization과 같은 moving average기법을 이용하는 네트워크에서는 control_dependencies를 걸어주므로써 같이 update가 되게 해줘야 한다.\n",
    "http://openresearch.ai/t/topic/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "if update_ops: \n",
    "     updates = tf.group(*update_ops)\n",
    "total_loss = control_flow_ops.with_dependencies([updates], total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dropout\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden layer의 개수가 많아질 경우 overfitting을 방지하기 위한 하나의 방법이다.\n",
    "- hidden layer중 dropout probability 만큼 랜덤하게 뉴런을 꺼서 학습을 하지 않게 만든다.\n",
    "- 일반적으로는 fc layer뒤에 넣지만 간혹 max pooling layer뒤에 넣는 경우도 있다.<br/>\n",
    "![dropout](picture/dropout.jpeg)\n",
    "<br/>\n",
    "출처 : https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/dropout_layer.html<br/>\n",
    "[참고하면 좋을 논문](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)<br/><br/>\n",
    "- 유의해야 할 사항은 predict 함수에서는 뉴런을 끄는 과정을 거치지 않고 p(dropout probability)값을 이용해서 layer의 값의 scale을 조절한다. 이 과정은 test하는 시간이 최소한 training time에서의 output을 구하는 시간과 동일해야하기 때문에 중요하다.<br/><br/>\n",
    "예를 들어, p = 0.5인 경우에는 뉴런은 test할 시에 출력을 반으로 줄여서 training에 걸리는 시간과 동일한 출력을 내놓아야 한다.<br/>\n",
    "만약 dropout을 적용하지 않았을 때 한 뉴런의 출력값이 x라고 한다면 dropout을 적용한 이후에 해당뉴런의 출력에 대한 기대값은 px+(1-p)0 이 된다. 왜냐하면 1-p의 확률로 뉴런의 output값은 0이 되기 때문이다.<br/>\n",
    "test할 시에 neuron은 항상 활성화상태로 유지해야하고 또 동일한 예상출력을 유지하기 위해서는 x를 px로 조정해야 한다.\n",
    "![dropouttest](picture/dropout_test.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_forward(x, dropout_param):\n",
    "    \"\"\"\n",
    "    Performs the forward pass for (inverted) dropout.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of any shape\n",
    "    - dropout_param: A dictionary with the following keys:\n",
    "      - p: Dropout parameter. We drop each neuron output with probability p.\n",
    "      - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "        if the mode is test, then just return the input.\n",
    "      - seed: Seed for the random number generator. Passing seed makes this\n",
    "        function deterministic, which is needed for gradient checking but not\n",
    "        in real networks.\n",
    "\n",
    "    Outputs:\n",
    "    - out: Array of the same shape as x.\n",
    "    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "      mask that was used to multiply the input; in test mode, mask is None.\n",
    "    \"\"\"\n",
    "    p, mode = dropout_param['p'], dropout_param['mode']\n",
    "    if 'seed' in dropout_param:\n",
    "        np.random.seed(dropout_param['seed'])\n",
    "\n",
    "    mask = None\n",
    "    out = None\n",
    "\n",
    "    if mode == 'train':\n",
    "        \n",
    "        mask = (np.random.rand(*x.shape) < p) / p\n",
    "        out = x*mask\n",
    "\n",
    "        pass\n",
    "\n",
    "    elif mode == 'test':\n",
    "        out = x\n",
    "\n",
    "        pass\n",
    "     \n",
    "\n",
    "    cache = (dropout_param, mask)\n",
    "    out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Perform the backward pass for (inverted) dropout.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: (dropout_param, mask) from dropout_forward.\n",
    "    \"\"\"\n",
    "    dropout_param, mask = cache\n",
    "    mode = dropout_param['mode']\n",
    "\n",
    "    dx = None\n",
    "    if mode == 'train':\n",
    "        \n",
    "        dx = dout * mask\n",
    "\n",
    "        pass\n",
    "\n",
    "    elif mode == 'test':\n",
    "        dx = dout\n",
    "    return dx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
