{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "- A set of methods that can automatically detect <u>patterns</u> in data and then use the uncovered patterns <u>to predict future</u> data, or to perform other kinds of decision making under uncertainty (Murphy)\n",
    "- Field of study that gives computers <u>the ability to learn</u> without being explicitly programmed (Auther Samuel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types\n",
    "- **Supervised Learning** : input과 output(label)이 둘다 주어져 있는경우, 목표는 input을 이용해서 output을 가장 잘 설명할 수 있는 general                              rule을 찾는것이다\n",
    "- **Unsupervised Learning** : label이 주어지 있지 않은 경우(ex, clustering, density estimation)\n",
    "- ** Reunformcement Learning** : 주변 환경에 따라서 software agent가 적응해 나가는 과정, 적절한 reward와 punishment가 주어진다. (ex,                                       markov decision process, dynamic programming) =>  [참고할만한 사이트](http://operatingsystems.tistory.com/159) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- Training data : model을 fitting하는데 사용하는 데이터 \n",
    "- Test(Independant) data : training data를 이용해 만든 model이 좋은 성능인지 test하는데 쓰는 데이터, training data에 포함되지 않는 데이터이여야 한다.\n",
    "- Overfitting problem : training data에 너무 적합하게 model을 만들어서 training에 사용되지 않는 많은 데이터(real data)에 model이 잘 fitting하지 않는 문제, y를 설명하는 general한 특징 뿐만 아니라 training data에만 적용하는 특징들(noise)가 많이 포함된 경우 발생한다.\n",
    "- Overfitting을 극복하는 3가지 방법\n",
    "  1) training data의 개수를 늘린다(좋은 데이터 셋이 포함되는 것도 중요하다)\n",
    "  2) Regularization\n",
    "  3) Bayesian Approach\n",
    "- Curse of dimensionality(차원의 저주) :  데이터의 차원이 증가할수록 설명해야 하는 공간의 크기(x들로 이루어진 모든 경우의 수)이 기하급수적으로 증가하기 때문에 샘플 데이터의 개수가 기하급수적으로 많이 필요로 한다. => [참고할만한 사이트](http://norman3.github.io/prml/docs/chapter01/4.html)\n",
    "<br>\n",
    "Example) $$ y=sin(2 \\pi x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##training data\n",
    "\"\"\"make training data with error 0.1 and follows normal_distribution\"\"\"\n",
    "x=np.random.rand(1,10)\n",
    "train=np.sin(2*np.pi*x)+(0.2*np.random.randn(1,10)-0.1)\n",
    "## test data\n",
    "\"\"\"make random value for test the model\"\"\"\n",
    "x_prime=np.random.rand(1,10)\n",
    "test=np.sin(2*np.pi*x_prime)+(0.2*np.random.randn(1,10)-0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1=np.linspace(0,1)\n",
    "plt.plot(x,train,marker='o',color='blue')\n",
    "plt.plot(x1,np.sin(2*np.pi*x1),color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=1\n",
    "w=np.polyfit(x.flatten(),train.flatten(),1)\n",
    "plt.plot(x,train,marker='o',color='blue')\n",
    "plt.plot(x1,np.sin(2*np.pi*x1),color='red')\n",
    "plt.plot(x1,np.polyval(w,x1),color='yellow')\n",
    "plt.title('Plots of polynomials having order 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "underfitting = high bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=3\n",
    "w=np.polyfit(x.flatten(),train.flatten(),3)\n",
    "plt.plot(x,train,marker='o',color='blue')\n",
    "plt.plot(x1,np.sin(2*np.pi*x1),color='red')\n",
    "plt.plot(x1,np.polyval(w,x1),color='yellow')\n",
    "plt.title('Plots of polynomials having order 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=9\n",
    "w=np.polyfit(x.flatten(),train.flatten(),9)\n",
    "plt.plot(x,train,marker='o',color='blue')\n",
    "plt.plot(x1,np.sin(2*np.pi*x1),color='red')\n",
    "plt.plot(x1,np.polyval(w,x1),color='yellow')\n",
    "plt.axis([-0.1, 1.1, -1.5, 1.5])\n",
    "plt.title('Plots of polynomials having order 9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting = high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## compare root mean square error\n",
    "train_RMSE=np.empty([9,1])\n",
    "test_RMSE=np.empty([9,1])\n",
    "for m in range(1,9):\n",
    "    w=np.polyfit(x.flatten(),train.flatten(),m)\n",
    "    yn_w=np.polyval(w,x)\n",
    "    yn_w_prime=np.polyval(w,x_prime)\n",
    "    \n",
    "    train_RMSE[m]=np.sqrt((np.sum(pow(yn_w-train,2)))/10)\n",
    "    test_RMSE[m]=np.sqrt(np.sum(pow(yn_w_prime-test,2))/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,10),train_RMSE,marker='o')\n",
    "plt.plot(range(1,10),test_RMSE,marker='o')\n",
    "plt.legend({'training','test'})\n",
    "plt.xlabel('M')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Root Mean Square Error of polynomial model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE getting smaller in training data but it gets higher in test data.\n",
    "So overfitting problem occurs when model dimensions gets high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Performance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "- Confusion matrix\n",
    "\n",
    "<img align=\"left\" src=\"picture/confusion.png\">\n",
    "\n",
    "$$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$$\n",
    " \n",
    "$$Sensitivity = \\frac{TP}{TP+FN}$$ \n",
    " \n",
    "$$Specificity = \\frac{TN}{FP+TN}$$ \n",
    " \n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    " \n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    " \n",
    "$$F-measure = \\frac{2*Precision*Recall}{Precision+recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Accuracy** : 실제 참인 것을 참으로 판단하고, 실제 거짓인 것을 거짓으로 판단하는 확률\n",
    "- **Sensitivity** : 실제 참인 것 중에서 참이라고 판단하는 확률\n",
    "- **Specificity** : 실제 거짓인 것 중에서 거짓이라고 판단하는 확률\n",
    "- **recall** : 실제 참인 것 중 참이라고 판단하는 확률(=Sensitivity)\n",
    "- **Precision** : 참이라고 예측한 것 중 진짜 참인 확률\n",
    "- **f-measure** : precision과 recall의 조화평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "\n",
    "예를 들어 암을 진단할 때, 성급한 의사는 아주 조금의 징후만 보여도 암인 것 같다고 할 것이다. 이 경우 TPR은 1에 가까워진다. 그러나 FRP은 매우 낮아져 버린다. 반대로 너무 신중한 의사는 대부분의 환자를 정상이라 진단할 것이다. 이경우 TPR은 0에 가까워지고 FRP은 1에 가까워질 것이다. 이처럼 TRP과 FRP은 둘다 어떤 기준(언제 참이라고 예측할 것인지)를 바꿔가면서 측정해야 한다. 이를 한눈에 표현한 것이 **ROC curve**이다. \n",
    "<br/>\n",
    "<img align=\"right\" src=\"picture/Roccurves.png\">\n",
    "- ROC(Receiver Operation Characteristic) Curve : Sensitivity(True Positive Rate)와 1-Specificity(False positive rate)로 그려지는 곡선<br/>\n",
    "    ROC분석은 optimal model을 고르고, class distribution과 관계없는 suboptimal model을 버리는 데 이용한다.<br/>\n",
    "    ROC curve 점이 왼쪽 위((0,1)지점)에 다가갈수록 좋은 성능이다. <br/><br/>\n",
    "    \n",
    "- AUC(Area under a ROC curve) : ROC curve의 밑 면적을 구한 값. 1에 가까울수록 좋은 성능을 나타낸다. auc가 0.5라는 말은 어떤 기준에서도 sensitivity와 specificity를 동시에 높일 수 있는 지점이 없음을 의미한다. \n",
    "\n",
    "<br/><img src = \"picture/roc_curves2.png\", align=\"left\">\n",
    "<br/><br/>\n",
    "- binary classification에서 X값들은 각각 어떠한 score값(decision function에 의해 결정을 내리기 위한 값)으로 변형이 되게 되고 어떤 Threshold paraemter T가 주어졌을 때, X>T 그 instance는 \"positive\"라고 예측되고, 반대의 경우에는 negative라고 예측된다.<br/>\n",
    "X는 만약 positive로 분류되었으면 f1이라는 확률 밀도함수를 따르고, negative로 분류되었으면 f0를 따른다고 가정했을 때,\n",
    "true positive rate은 $ TPR(T)=\\int_{T}^{\\infty}f_1(x)dx $, false positive rate은 $ FPR(T)=\\int_{T}^{\\infty}f_1(x)dx$와 같아지게 된다.<br/> ROC curve는 T를 바꿔가면서 TPR(T) 와 FPR(T)의 비율을 조절한다. <br/><br/>\n",
    "예를 들어 정상 환자들과 질병이 있는 환자들의 혈당량이 각각 평균 2g/dL과 1g/dL의 정규분포를 따른다고 가정하자.<br/> 몇몇 sample들의 혈당검사를 통해서 혈당량이 얼마 이상이 되었을 때(threshold), 질병을 가진다고 나타내는 지표가 되는지 알아보고자 한다.<br/> 실험하는 사람들은 threshold를 조절해가면서 이를 조정하게 되고 이에 따라 false positive rate이 바뀐다. <br/>\n",
    "만약 threshold가 증가하면 false positive가 줄어 들 것이고, 그림에서 검은선(threshold)가 오른쪽으로 이동할 것이다. 실제 curve의 모양은 두 분포가 얼마나 overlap 되어있는지에 따라 결정된다.<br/><br/>\n",
    "[참고](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)      [참고2](https://kennis-research.shinyapps.io/ROC-Curves/)<br/>\n",
    "The ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from - ∞ to + ∞ ) of the correct detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability in x-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve\n",
    "\n",
    "positive와 negative class의 개수가 imbalance할 때는 주로 Precision Recall curve를 이용한다.<br/><br/>\n",
    "예를 들어, 문서에서 특정 주제와 관련된 정보를 찾고자 할 때 실제 관련 있는 문서의 개수가 100개, 관련 없는 문서의 개수가 1,000,000개라고 하자.<br/>\n",
    "<img align=\"right\" src=\"picture/PR-curve.png\">\n",
    "\n",
    "이때 알고리즘1은 100개의 문서를 참이라고 판단하였고 그 중 90개의 실제 관련 있는 문서를 찾아내었다고 하자.<br/> 즉, TP=90, TN=999890, FP=10, FN = 10이다.<br/> 반면 알고리즘2는 2000개의 문서를 참이라고 하였고, 그 중 90개의 실제 관련있는 문서를 찾아내었다고 하자.<br/> 이 경우에는 TP=90, TN=997990, FP=1910, FN=10이다. 이 경우세 명백히 알고리즘 1이 더 좋은 성능을 보인다고 할 수 있다.<br/>\n",
    "\n",
    "\n",
    "이 경우 TPR과 FRP을 계산해 보면,\n",
    "- algo1 : TPR = TP/(TP+FN) = 90/(90+10) = 0.9      // FPR = FP/(FP+TN)=10/(10+999890)=0.00001\n",
    "- algo2 : TPR = TP/(TP+FN) = 90/(90+10) = 0.9      // FPR = FP/(FP+TN)=1910/(1910+997990)=0.0019\n",
    "이 경우 FPR은 두 알고리즘의 차이를 크게 반영하지 못한다. <br/>\n",
    "\n",
    "precision과 recall을 계산하여 본다면,\n",
    "- algo1 : Precision=TP/(TP+FP) = 90/(90+10) =0.9     //  Recall=TP/(TP+FN)=90/(90+10)=0.9\n",
    "- algo2 : Precision=TP/(TP+FP) = 90/(90+1910) =0.045     //  Recall=TP/(TP+FN)=90/(90+10)=0.9\n",
    "Precision을 이용하면 상대적인 불균형을 명백히 반영한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Precision-Recall curve : precision과 recall값의 변화를 그래프로 표현한 것\n",
    "\n",
    "- AUPR(Area under Pr Curve) : precision-recall curve의 면적 \n",
    "\n",
    "[참고](http://www.chioka.in/differences-between-roc-auc-and-pr-auc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import RandomState\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breast_data = pd.read_table('breast_cancer.csv',sep=',')\n",
    "breast_data=breast_data.drop('Unnamed: 32',axis=1)\n",
    "X=breast_data.iloc[:,2:]\n",
    "y=breast_data.iloc[:,1]\n",
    "y=label_binarize(y,classes=['B','M'])      #Malignant -> 1 , benign -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4,random_state=RandomState(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier=LogisticRegression(penalty='l2',random_state=RandomState(None))\n",
    "\n",
    "logistic=classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_score=logistic.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr,tpr,threshold = roc_curve(y_test,y_score,pos_label=1)\n",
    "roc_auc=auc(fpr,tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw=2\n",
    "plt.plot(fpr,tpr,color='darkorange',lw=lw,label='ROC curve (area=%0.2f)' % roc_auc)\n",
    "plt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regression\n",
    "\n",
    "* RMSE(Root Mean Squared Error)    \n",
    "$$ RMSE = \\sqrt{\\sum \\frac{(y_{pred}-y_{ref})^2}{N}} $$\n",
    "\n",
    "\n",
    "* MAD(Mean Squared Deviation)  = MAE(Mean Squared Error)\n",
    "$$ MAD =  \\frac{1}{n}\\sum\\left | y_{pred}-y_{ref}\\right |  $$\n",
    "\n",
    "\n",
    "* Pearson Correlation Coefficient \n",
    "![equation](picture/pearson.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "## Validation\n",
    "\n",
    "### Data set\n",
    "\n",
    "- **Training set** : model을 학습하는 데 필요로 하는 데이터<br/><br/>\n",
    "\n",
    "- **Validation set** : model의 parameter를 정하거나 model의 종류를 고를 때와 같이 최적화를 할 때 필요로 하는 데이터 셋. training set과 겹쳐도 되지만 거의 겹치지 않는 데이터 셋이 좋다<br/><br/>\n",
    "\n",
    "- **Test set(Independent Set)** : training set에 속하지 않는 데이터로써, model의 성능을 평가하는 데 이용하는 데이터<br/>\n",
    "\n",
    "### n- fold Cross Validation ( Internal validation)\n",
    "- training set을 n조각으로 나눈 뒤 n-1개의 조각으로 training 한 모델을 남은 1조각에 test해보는것\n",
    "- validation set을 구하기 힘들 때 parameter selection이나 model tuning에도 많이 이용된다.\n",
    "<img src=\"picture/cross_validation.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_cv(n,model,data):\n",
    "    \n",
    "    pred=pd.DataFrame()\n",
    "    real=pd.DataFrame()\n",
    "    \n",
    "    data_t=data.iloc[:,1:].transpose()\n",
    "    \n",
    "    kf=KFold(n_splits=n,shuffle=True)\n",
    "    prediction=pd.DataFrame(columns=['real','pred'])\n",
    "    \n",
    "    y_data=data_t.iloc[:,-1]\n",
    "    y_data=make_binary('normal','cancer',y_data).values\n",
    "    \n",
    "    \n",
    "    for train, test in kf.split(data_t):\n",
    "        \n",
    "        x_train=data_t.iloc[train,:-1].astype('float64').values\n",
    "        y_train=y_data.iloc[train,-1].values\n",
    "        model.fit(x_train,y_train)\n",
    "    \n",
    "        x_test=data_t.iloc[test,:-1].astype('float64').values\n",
    "        y_test=y_data.iloc[test,-1].values        \n",
    "         \n",
    "        pred=pred.append(pd.DataFrame(model.predict(x_test)))\n",
    "        real=real.append(pd.DataFrame(y_test))\n",
    "                \n",
    "    prediction=pd.concat([pred,real],axis=1)\n",
    "\n",
    "    acc=accuracy(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    prec=precision(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    rec=recall(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    f1=f1_score(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    \n",
    "    print('accuracy : '+str(acc)+'\\nprecision :'+str(prec)\n",
    "         +'\\nrecall : '+str(rec)+'\\nf1_score : '+str(f1))\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_cv(n,model,data):\n",
    "    \n",
    "    pred=pd.DataFrame()\n",
    "    real=pd.DataFrame()\n",
    "    \n",
    "    data_t=data.iloc[:,1:].transpose()\n",
    "    \n",
    "    kf=KFold(n_splits=n,shuffle=True)\n",
    "    prediction=pd.DataFrame(columns=['real','pred'])\n",
    "        \n",
    "    for train, test in kf.split(data_t):\n",
    "        \n",
    "        x_train=data_t.iloc[train,:-1].astype('float64').values\n",
    "        y_train=data_t.iloc[train,-1].astype('float64').values\n",
    "        model.fit(x_train,y_train)\n",
    "    \n",
    "        x_test=data_t.iloc[test,:-1].astype('float64').values\n",
    "        y_test=data_t.iloc[test,-1].astype('float64').values        \n",
    "         \n",
    "        pred=pred.append(pd.DataFrame(model.predict(x_test)))\n",
    "        real=real.append(pd.DataFrame(y_test))\n",
    "                \n",
    "    prediction=pd.concat([pred,real],axis=1)\n",
    "\n",
    "    rmse=rmse_cal(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    mad=mae_cal(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    cor=cor_cal(prediction.iloc[:,0],prediction.iloc[:,1])\n",
    "    \n",
    "    print('rmse : '+str(rmse)+'\\nmad : '+str(mad)+'\\ncor : '+str(cor[0]))\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regression_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-51e9cb5885e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcv_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregression_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regression_cv' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from tools import rmse_cal,mae_cal,cor_cal,mean_cal,frange,accuracy,precision,recall,aupr,f1_score,make_binary\n",
    "\n",
    "dataset=pd.read_table('data/blood_age_selected_lasso.tsv',sep='\\t')\n",
    "\n",
    "linear=LinearRegression()\n",
    "\n",
    "cv_result=regression_cv(10,regression,linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.KNeighborsClassifier\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
