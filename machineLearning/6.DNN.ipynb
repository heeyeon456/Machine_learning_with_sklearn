{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Artifical Neural Network\n",
    "- biological neural network를 모방한 computing system이다.\n",
    "- layer가 두개 이상 쌓인 형태는 Deep Neural Network라고 한다.\n",
    "- 간단히 생각하면 아래 그림과 같이 linear classifier를 여러번 이어놓은 형태를 의미한다. <br/>hidden node에서 input의 feature들을 우리가 지정한 hidden layer의 개수만큼으로 표현해주고 이러한 linear function을 여러번 만들어서 최종 output을 추출해내는 형태이다. <br/>\n",
    "    이때 neural network가 어떤 식으로 feature들을 가공하는지 알 수 없기 때문에 blackbox algorithm이라고 하며, 기계가 사람이 알 수 없는 방식으로 필요한 feature들을 뽑아준다는 점에서 deep learning이 기존의 머신러닝 알고리즘들과 가장 큰 차이점을 보인다고 할 수 있다.<br/> \n",
    "![nn](picture/neural_network.png)\n",
    "<br/>\n",
    "- 그림에서 보이는 각각의 선들이 weight를 의미한다고 할 수 있다.<br/> Training과정에서 input을 넣어서 output을 계산하고 score를 계산하는 과정을 **forward propagation** 이라고 하며, loss function을 이용해서 weight를 loss를 줄이는 방향으로 update를 하는 과정을 **back propagation**이라고 한다.<br/> 이러한 과정으로 한번의 forward propagation이랑 back propagation 과정을 한번 거치는 것을 1 **epoch**라고 한다.<br/><br/>\n",
    "- deep learning model은 다양한 hyperparameter에 의해서 영향을 받는다. 이러한 hyperparamter의 종류는 다음과 같다.\n",
    "    - learning rate : weight를 얼마만큼 이동하면서 update할 것인지\n",
    "    - number of hidden layer : layer를 얼마나 쌓을 것인지\n",
    "    - number of hidden node for each layer : 각 hidden layer의 node를 몇개씩 지정해 줄것인지\n",
    "    - regularization : l2또는 l1 regularization 을 얼마만큼 적용해줄 것인지\n",
    "    - dropout rate : dropout시키는 비율을 얼마로 할것인지\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network의 원리\n",
    "## Forward pass\n",
    "\n",
    "input으로 (N,D)의 X가 주어졌다고 했을 때, hidden layer가 하나인 forward pass의 계산과정은 다음과 같다. <br/>\n",
    "각 layer마다 linear classifier와 동일하게 wx+b 함수를 계산하고, activation함수를 이용해서 output이 0또는 1나올 수 있게 조정해준다.<br/>\n",
    "여기서는 activation function으로 ReLU함수를 이용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(self,x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "# parameter W와 b 불러오기\n",
    "W1, b1 = self.params['W1'], self.params['b1']\n",
    "W2, b2 = self.params['W2'], self.params['b2']\n",
    "N, D = X.shape\n",
    "\n",
    "# Compute the forward pass \n",
    "scores = None\n",
    "u1 = X.dot(W1)+b1\n",
    "z1 = self.relu(u1)\n",
    "scores = z1.dot(W2)+b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute loss\n",
    "계산해서 얻은 output이 실제랑 얼마나 잘 맞는지(해당 모델이 얼마나 잘 학습되었는지 확인하기 위해 loss를 계산한다.<br/>\n",
    "loss function으로는 대표적으로 svm loss(hinge loss)와 softmax function을 많이 이용한다.<br/>\n",
    "여기서는 svm loss를 활용하였다. 그리고 loss에 regularization term을 더해줌으로써 overfitting을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the loss\n",
    "loss = None\n",
    "        \n",
    "prob = np.exp(scores)/np.sum(np.exp(scores),axis=1,keepdims=True)\n",
    "        \n",
    "loss = np.sum(-np.log(prob[range(N),y]))/N\n",
    "loss += 0.5 * reg * (np.sum(W1**2)+np.sum(W2**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax loss function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_train = X.shape[0]\n",
    "    num_classes = W.shape[1]\n",
    "    \n",
    "    score=X.dot(W)\n",
    "    stable_score=score-np.array(num_classes*[np.max(score,axis=1),]).T\n",
    "    prob = np.exp(stable_score)/np.sum(np.exp(stable_score),axis=1,keepdims=True)\n",
    "    \n",
    "    loss=np.sum(-np.log(prob[np.arange(0,prob.shape[0]),y]))\n",
    "    loss=loss/num_train + 0.5 * reg * np.sum(W*W)  \n",
    "    \n",
    "    prob[np.arange(0,prob.shape[0]),y] -= 1\n",
    "    \n",
    "    dW = prob.T.dot(X).T\n",
    "    \n",
    "    dW /= num_train\n",
    "    dW += reg*W\n",
    "\n",
    "    pass\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVM Loss **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "    Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "    pass\n",
    "  \n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    scores = X.dot(W)\n",
    "\n",
    "    correct=scores[np.arange(0,scores.shape[0]),y]\n",
    "    #dW[:,y[i]]+=-2*X[i]\n",
    "        \n",
    "    correct_score=np.array([correct,]*num_classes).T\n",
    "    \n",
    "    margin=scores-correct_score+1\n",
    "    loss=np.sum(margin*(margin>0)*(margin!=1))/num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "     \n",
    "    pass\n",
    "\n",
    "    sum_or_not=np.array((margin>0)*(margin!=1),dtype=float)\n",
    "    sum_or_not[np.arange(0,scores.shape[0]),y]=-np.sum(sum_or_not,axis=1)\n",
    "    \n",
    "    dW=np.dot(X.T,sum_or_not)\n",
    "    \n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "    \n",
    "    pass\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "back propagation은 gradient descent optimization방법을 이용해서 weight를 update해나가는 과정을 의미한다 <br/>\n",
    "**Gradient Descent**란 가설함수의 local minimum지점을 찾기 위해서 현재 지점의 gradient(미분값)의 음수값에 비례하게 이동해가면서 minimum을 찾아나가는 optimization algorithm이다.  \n",
    "![sgd](picture/sgd.png)\n",
    "따라서 각각의 parameter(W1, b1, W2, b2)에 대해서 weight를 update하기 위해서는 loss function에 대해서 각각의 parameter에 대한 편미분(partial derivative)값을 구해줌으로써 그 값에 비례하게 weight를 update한다.\n",
    "$$ new_w = w - (learning\\,rate)* \\frac{\\delta\\,loss}{\\delta \\,w} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward pass: compute gradients\n",
    "dscore = prob.copy()\n",
    "dscore[np.arange(0,prob.shape[0]),y] -= 1\n",
    "dscore = dscore/N\n",
    "        \n",
    "grads = {}\n",
    "grads['W2'] = z1.T.dot(dscore) + reg*W2\n",
    "grads['b2'] = np.sum(dscore,axis=0) + reg*b2\n",
    "       \n",
    "tmp = dscore.dot(W2.T)\n",
    "tmp[z1<=0]=0\n",
    "grads['W1'] = X.T.dot(tmp) +reg*W1\n",
    "grads['b1'] = np.sum(tmp,axis=0) + reg*b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 과정을 여러 번(iteration) 반복해주면 weight가 training data를 잘 설명해 줄 수 있는 값으로 최적화되고 training과정이 끝나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tools import rmse_cal,mae_cal,cor_cal,mean_cal,frange,\\\n",
    "                    accuracy,precision,recall,aupr,f1_score,make_binary\n",
    "from validation import classification_cv,regression_cv,external_val_reg,\\\n",
    "                        external_val_classif, test_preprocessing, \\\n",
    "                        draw_roc,cal_external_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "dataset=pd.read_table('data/breast_cancer_svc.tsv',sep='\\t')\n",
    "\n",
    "input_data=dataset.iloc[:,1:].transpose()\n",
    "X_data=input_data.iloc[:,:-1].values\n",
    "y_data=input_data.iloc[:,-1]\n",
    "y_data=make_binary('normal','cancer',y_data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.1, random_state=42)\n",
    "\n",
    "y_train = enc.fit_transform(y_train.values.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 2)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variable 정의\n",
    "X_dim = X_train.shape[1]\n",
    "input_n = X_train.shape[0]\n",
    "hidden_1 = 200\n",
    "hidden_2 = 100\n",
    "output_size = 2\n",
    "lr = 5e-04\n",
    "max_epoch = 50\n",
    "dropout_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create fully connected layer\n",
    "def dense(input_layer,layer_size):\n",
    "    \n",
    "    init = tf.contrib.layers.xavier_initializer()\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "    \n",
    "    # Batchnorm settings\n",
    "    #training_phase = tf.placeholder(tf.bool, phase, name='training_phase')\n",
    "    \n",
    "    HiddenLayer = tf.layers.dense(inputs = input_layer, units = layer_size, \n",
    "                              activation=None, # Batchnorm comes before nonlinear activation\n",
    "                              use_bias=False, # Note that no bias unit is used in batchnorm\n",
    "                              kernel_initializer=init, kernel_regularizer = regularizer)\n",
    "    \n",
    "    HiddenLayer = tf.layers.batch_normalization(HiddenLayer)\n",
    "    HiddenLayer = tf.nn.relu(HiddenLayer)\n",
    "    \n",
    "    return HiddenLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# input 형태 정의\n",
    "X = tf.placeholder(tf.float32, [None, X_dim])\n",
    "y = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "with tf.name_scope(\"DNN\"):\n",
    "    \n",
    "    keep_proba = tf.placeholder(tf.float32, None, name='keep_proba')\n",
    "\n",
    "    \n",
    "    hidden_layer1 = dense(X,hidden_1)\n",
    "    hidden_layer2 = dense(hidden_layer1, hidden_2)\n",
    "    hidden_layer2 = tf.nn.dropout(hidden_layer2, keep_prob=dropout_prob)\n",
    "    output_layer = dense(hidden_layer2, output_size)\n",
    "     \n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=output_layer,labels=y)\n",
    "    cost = tf.reduce_mean(loss, name = 'loss')\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "    training = optimizer.minimize(loss,name='training')\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(tf.argmax(y,1), tf.argmax(output_layer,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name='eval')\n",
    "    \n",
    "    \n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 | Training Accuracy: 0.866009   Validation Accuracy: 0.873418\n",
      "Epoch :  2 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  3 | Training Accuracy: 0.874471   Validation Accuracy: 0.911392\n",
      "Epoch :  4 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  5 | Training Accuracy: 0.874471   Validation Accuracy: 0.911392\n",
      "Epoch :  6 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  7 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  8 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  9 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  10 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  11 | Training Accuracy: 0.875881   Validation Accuracy: 0.911392\n",
      "Epoch :  12 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  13 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  14 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  15 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  16 | Training Accuracy: 0.875881   Validation Accuracy: 0.911392\n",
      "Epoch :  17 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  18 | Training Accuracy: 0.875882   Validation Accuracy: 0.924051\n",
      "Epoch :  19 | Training Accuracy: 0.877292   Validation Accuracy: 0.911392\n",
      "Epoch :  20 | Training Accuracy: 0.878702   Validation Accuracy: 0.911392\n",
      "Epoch :  21 | Training Accuracy: 0.878702   Validation Accuracy: 0.911392\n",
      "Epoch :  22 | Training Accuracy: 0.878702   Validation Accuracy: 0.911392\n",
      "Epoch :  23 | Training Accuracy: 0.875882   Validation Accuracy: 0.911392\n",
      "Epoch :  24 | Training Accuracy: 0.877292   Validation Accuracy: 0.911392\n",
      "Epoch :  25 | Training Accuracy: 0.881523   Validation Accuracy: 0.911392\n",
      "Epoch :  26 | Training Accuracy: 0.881523   Validation Accuracy: 0.911392\n",
      "Epoch :  27 | Training Accuracy: 0.888576   Validation Accuracy: 0.924051\n",
      "Epoch :  28 | Training Accuracy: 0.891396   Validation Accuracy: 0.911392\n",
      "Epoch :  29 | Training Accuracy: 0.885755   Validation Accuracy: 0.911392\n",
      "Epoch :  30 | Training Accuracy: 0.891396   Validation Accuracy: 0.936709\n",
      "Epoch :  31 | Training Accuracy: 0.894217   Validation Accuracy: 0.936709\n",
      "Epoch :  32 | Training Accuracy: 0.894217   Validation Accuracy: 0.949367\n",
      "Epoch :  33 | Training Accuracy: 0.906911   Validation Accuracy: 0.936709\n",
      "Epoch :  34 | Training Accuracy: 0.909732   Validation Accuracy: 0.924051\n",
      "Epoch :  35 | Training Accuracy: 0.906911   Validation Accuracy: 0.924051\n",
      "Epoch :  36 | Training Accuracy: 0.915374   Validation Accuracy: 0.962025\n",
      "Epoch :  37 | Training Accuracy: 0.912553   Validation Accuracy: 0.949367\n",
      "Epoch :  38 | Training Accuracy: 0.915374   Validation Accuracy: 0.949367\n",
      "Epoch :  39 | Training Accuracy: 0.916784   Validation Accuracy: 0.949367\n",
      "Epoch :  40 | Training Accuracy: 0.925247   Validation Accuracy: 0.949367\n",
      "Epoch :  41 | Training Accuracy: 0.923836   Validation Accuracy: 0.936709\n",
      "Epoch :  42 | Training Accuracy: 0.928068   Validation Accuracy: 0.936709\n",
      "Epoch :  43 | Training Accuracy: 0.928068   Validation Accuracy: 0.949367\n",
      "Epoch :  44 | Training Accuracy: 0.929478   Validation Accuracy: 0.962025\n",
      "Epoch :  45 | Training Accuracy: 0.93371   Validation Accuracy: 0.949367\n",
      "Epoch :  46 | Training Accuracy: 0.93512   Validation Accuracy: 0.949367\n",
      "Epoch :  47 | Training Accuracy: 0.93371   Validation Accuracy: 0.962025\n",
      "Epoch :  48 | Training Accuracy: 0.93512   Validation Accuracy: 0.949367\n",
      "Epoch :  49 | Training Accuracy: 0.93653   Validation Accuracy: 0.949367\n",
      "Epoch :  50 | Training Accuracy: 0.947814   Validation Accuracy: 0.974684\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    while epoch_count < max_epoch :\n",
    "        \n",
    "        sess.run(training,feed_dict={X:X_train,y:y_train})\n",
    "        accu_train = accuracy.eval(feed_dict={X:X_train,y:y_train})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:X_test,y:y_test})\n",
    "        \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            \n",
    "        epoch_count+=1\n",
    "        \n",
    "        print('Epoch : ',epoch_count, '| Training Accuracy:',accu_train,'  Validation Accuracy:',accu_valid)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = 128\n",
    "hidden_2 = 64\n",
    "dropout_rate = 0.5\n",
    "batch_size = 25\n",
    "\n",
    "epochs = 20\n",
    "train_samples = X_train.shape[0]\n",
    "validation_samples = X_test.shape[0]\n",
    "x_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 294)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(hidden_1,activation = 'relu',input_dim = x_dim))\n",
    "model.add(Dense(hidden_2, activation = 'relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(2, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 709 samples, validate on 79 samples\n",
      "Epoch 1/20\n",
      "709/709 [==============================] - 1s 1ms/step - loss: 0.3610 - acc: 0.8717 - val_loss: 0.2391 - val_acc: 0.9114\n",
      "Epoch 2/20\n",
      "709/709 [==============================] - 0s 617us/step - loss: 0.2789 - acc: 0.8942 - val_loss: 0.2002 - val_acc: 0.9557\n",
      "Epoch 3/20\n",
      "709/709 [==============================] - 0s 618us/step - loss: 0.2202 - acc: 0.9281 - val_loss: 0.1987 - val_acc: 0.9620\n",
      "Epoch 4/20\n",
      "709/709 [==============================] - 0s 608us/step - loss: 0.1760 - acc: 0.9337 - val_loss: 0.0994 - val_acc: 0.9747\n",
      "Epoch 5/20\n",
      "709/709 [==============================] - 0s 617us/step - loss: 0.1116 - acc: 0.9633 - val_loss: 0.0663 - val_acc: 0.9747\n",
      "Epoch 6/20\n",
      "709/709 [==============================] - 0s 606us/step - loss: 0.0737 - acc: 0.9810 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "709/709 [==============================] - 0s 607us/step - loss: 0.0562 - acc: 0.9838 - val_loss: 0.0597 - val_acc: 0.9747\n",
      "Epoch 8/20\n",
      "709/709 [==============================] - 0s 620us/step - loss: 0.0332 - acc: 0.9972 - val_loss: 0.0517 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "709/709 [==============================] - 0s 604us/step - loss: 0.0308 - acc: 0.9937 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "709/709 [==============================] - 0s 602us/step - loss: 0.0192 - acc: 0.9965 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "709/709 [==============================] - 0s 610us/step - loss: 0.0150 - acc: 0.9993 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "709/709 [==============================] - 0s 610us/step - loss: 0.0103 - acc: 0.9993 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "709/709 [==============================] - 0s 629us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "709/709 [==============================] - 0s 615us/step - loss: 0.0096 - acc: 0.9993 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "709/709 [==============================] - 0s 627us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "709/709 [==============================] - 0s 604us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "709/709 [==============================] - 0s 601us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "709/709 [==============================] - 0s 626us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "709/709 [==============================] - 0s 614us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "709/709 [==============================] - 0s 617us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fb1e75160>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# should install h5py\n",
    "\n",
    "model.save_weights('log/dnn_weight1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
