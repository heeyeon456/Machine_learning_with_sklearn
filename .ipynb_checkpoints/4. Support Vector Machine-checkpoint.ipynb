{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### lagme margin classification\n",
    "- logistic regression vs. support vector machine<br/>\n",
    "<img src=\"picture/svm1.png\">\n",
    "cost1함수(hyplerplane함수값) 값이 1보다 작으면 1로 분류되고, -1보다 작으면 0으로 분류된다.<br/><br/>\n",
    "y(i)=1 -> cost1(z) >= 1    \n",
    "y(i)=0 -> cost0(z) <= -1\n",
    "<img src=\"picture/svm2.png\">\n",
    "- large margin classification\n",
    "classification problem에서 svm은 class간의 가장 margin이 큰 hyperplane을 찾고자 한다.<br/>\n",
    "여기서 **margin**은 hyperplane과 각 class의 hyperplane과 가장 가까운 element와의 거리를 의미한다.<br/><br/>\n",
    "<img src =\"picture/svm3.png\"><br/>\n",
    "C가 크면 regularization이 적고, 모든 데이터와 outlier까지 모두 고려해서 가장 적합한 hyperplane을 찾고자 한다.<br/>\n",
    "C가 작으면 outlier는 penalize시킬려는 경향이 강하고, regularization이 많이 된다. <br/>\n",
    "margin을 최대화하는 지점을 찾기 위해서는 w 값을 minimize시켜야 한다!\n",
    "<br/><br/>\n",
    "### Decision boundary\n",
    "* 참고\n",
    "<img src = \"picture/inner_project.png\">\n",
    "<br/><br/>\n",
    "왜 w(parameter vector)를 minimizing시키는 지점이 margin이 최대화하는 hyperplane인가<br/><br/>\n",
    "결국 svm에서는 데이터 사이의 margin을 최대화시키면서도 general한 hyperplane을 찾고자 한다. <br/><br/>\n",
    "일단 C(regularizing factor)는 고려하지 않고, 데이터만을 고려했을 때 hyperplane에 해당하는 vector를 $g$라고 하자<br/>\n",
    "$ \\vec{g} = \\sum_i^k\\theta_ix_i+\\theta_0 $ 이 경우 단순하게 생각하기 위해 $ \\theta_0 = 0, k(feature\\,number) = 0 $이라 하자<br/><br/>\n",
    "그렇다면 이 경우에서는 아래와 같은 optimization problem을 해결하고자 한다.<br/><br/>\n",
    "$$ min_{\\theta}\\,\\frac{1}{2}\\sum_{j=1}^n\\theta_j^2 = \\frac{1}{2}(\\theta_1^2+\\theta_2^2) = \\frac{1}{2}(\\sqrt{\\theta_1^2+\\theta_2^2})^2 =  \\frac{1}{2}\\left \\| \\theta \\right \\|^2 $$ <br/>\n",
    "<img src=\"picture/svm4.png\" align = \"right\">\n",
    "$ s.t.$ <br/>\n",
    "$ \\theta^Tx(i)\\geq 1\\,,\\,if\\, y^{(i)}=1, $ <br/><br/>\n",
    "$ \\theta^Tx(i)\\leq -1\\,,\\,if\\, y^{(i)}=0 $ <br/><br/><br/>\n",
    "여기서 $\\theta^Tx(i) $는 무엇을 의미할까.<br/><br/>\n",
    "$= p^{(i)} \\cdot \\left \\| \\theta \\right \\| $ <br/><br/>\n",
    "$= \\theta_1x_1^{(i)}+\\theta_2x_2^{(i)} $\n",
    "<img src=\"picture/svm5.png\">\n",
    "초록색을 support vector(hyperplane)이라고 가정하면, <br/>hyperplane vector g의 orthogonal vector $\\theta$인($\\theta_0$가 0이므로 g의 projection vector는 $\\theta$이다.) <br/> 각 class에서 가장 가까운 element를 projection했을 때의 길이를 $p^{(i)}$라 했을 때, 왼쪽 그래프의 경우 hyperplane이 두 class를 적절하게 분리하지 못하고 있기 때문에, $p(i)$가 작은 값이고, 두 class를 분리하기 위해서는 $\\left \\| \\theta \\right \\| $ 가 큰 값이 되게 된다. <br/>하지만 오른쪽 그래프의 경우 $p(i)$가 큰 값이고, $\\left \\| \\theta \\right \\| $ 가 작은 값이 된다. 오른쪽 그래프가 더욱더 큰 margin을 가지고 classify 하는 hyperplane이라고 할 수 있다.<br/><br/>\n",
    "따라서 최소의 parameter vector값을 구하면 margin을 최대화하는 supprot vector를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
